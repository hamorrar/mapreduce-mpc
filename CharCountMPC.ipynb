{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "### IMPORTS\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import asyncio\n",
        "\n",
        "!pip install mpyc\n",
        "from mpyc.runtime import mpc"
      ],
      "metadata": {
        "id": "Noxqu4vei6b6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TRAIN AND SAVE MODEL\n",
        "train_csv_path = \"./data/training_set.csv\"\n",
        "test_csv_path = \"./data/test_set.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_csv_path, delimiter=';')\n",
        "test_df = pd.read_csv(test_csv_path, delimiter=';')\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5)\n",
        "\n",
        "max_length = 128\n",
        "train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n",
        "test_encodings = tokenizer(test_df['text'].tolist(), truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n",
        "\n",
        "train_input_ids = np.array(train_encodings['input_ids'])\n",
        "train_token_type_ids = np.array(train_encodings['token_type_ids'])\n",
        "train_attention_mask = np.array(train_encodings['attention_mask'])\n",
        "\n",
        "test_input_ids = np.array(test_encodings['input_ids'])\n",
        "test_token_type_ids = np.array(test_encodings['token_type_ids'])\n",
        "test_attention_mask = np.array(test_encodings['attention_mask'])\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encodings_train = label_encoder.fit_transform(train_df['label'])\n",
        "label_encodings_test = label_encoder.transform(test_df['label'])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "history = model.fit(\n",
        "    x={'input_ids': train_input_ids, 'token_type_ids': train_token_type_ids, 'attention_mask': train_attention_mask},\n",
        "    y=label_encodings_train,\n",
        "    epochs=3,\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(\n",
        "    x={'input_ids': test_input_ids, 'token_type_ids': test_token_type_ids, 'attention_mask': test_attention_mask},\n",
        "    y=label_encodings_test,\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Test Accuracy: {test_accuracy:.2%}')\n",
        "\n",
        "model.save_pretrained(\"best_model\")"
      ],
      "metadata": {
        "id": "enWHgRQiSfjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MPC version of Chararacter Counting Example\n",
        "async def mainmpc(file_path):\n",
        "    secint = mpc.SecInt(16)\n",
        "\n",
        "    await mpc.start()\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    ascii_values = [secint(ord(char)) for char in text]\n",
        "\n",
        "    count = mpc.input(secint(len(ascii_values)))\n",
        "\n",
        "    print('Number of Characters:', await mpc.output(count))\n",
        "    await mpc.shutdown()\n",
        "\n",
        "# Regular version of Character Counting Example\n",
        "def mainreg(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    ascii_values = [ord(char) for char in text]\n",
        "\n",
        "    count = len(ascii_values)\n",
        "\n",
        "    print('Number of Characters:', count)\n",
        "\n",
        "# Make prediction based on tuned BERT model from above\n",
        "def new_predictions(file_path, k):\n",
        "\n",
        "  loaded_model = TFBertForSequenceClassification.from_pretrained(\"best_model\")\n",
        "  tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "  text_df = pd.read_csv(file_path, delimiter=';', nrows=k)\n",
        "\n",
        "  max_length = 128\n",
        "  new_encodings = tokenizer(text_df['text'].tolist(), truncation=True, padding=True, max_length=max_length, return_tensors='tf')\n",
        "\n",
        "  new_input_ids = np.array(new_encodings['input_ids'])\n",
        "  new_token_type_ids = np.array(new_encodings['token_type_ids'])\n",
        "  new_attention_mask = np.array(new_encodings['attention_mask'])\n",
        "\n",
        "  predictions = loaded_model.predict(\n",
        "      x={'input_ids': new_input_ids, 'token_type_ids': new_token_type_ids, 'attention_mask': new_attention_mask}\n",
        "  )\n",
        "\n",
        "  predicted_labels = np.argmax(predictions.logits, axis=1)\n",
        "\n",
        "  # Uncomment if needed, but these are correct labels for the specified index. Not sure how/why the order is set.\n",
        "  # decode_label = {}\n",
        "  # decode_label[0] = \"Health\"\n",
        "  # decode_label[1] = \"Other\"\n",
        "  # decode_label[2] = \"Politics\"\n",
        "  # decode_label[3] = \"Religion\"\n",
        "  # decode_label[4] = \"Sexuality\"\n",
        "\n",
        "  return predicted_labels\n",
        "\n",
        "# Read the first k lines of a CSV file and return\n",
        "def read_first_x_lines(file_path, k):\n",
        "    with open(file_path, 'r') as file:\n",
        "        first_k_lines = []\n",
        "\n",
        "        for _ in range(k):\n",
        "            line = file.readline().strip()\n",
        "            first_k_lines.append(line)\n",
        "\n",
        "    return first_k_lines\n",
        "\n",
        "# Get first k lines, make predictions on them to determine if MPC is needed or not\n",
        "def main(file_path, threshold, k):\n",
        "  first_k_list = read_first_x_lines(file_path, k)\n",
        "\n",
        "  if len(first_k_list) < k:\n",
        "    print(\"File is empty or contains fewer than \", k, + \" lines. Try different value for k.\")\n",
        "    return\n",
        "\n",
        "  preds = new_predictions(file_path, k)\n",
        "\n",
        "  count_sens = 0\n",
        "  for x in preds:\n",
        "    if x != 1:\n",
        "      count_sens += 1\n",
        "\n",
        "  # if there is more than threshold% of sensitive info in the first k lines sampled, use MPC\n",
        "  if int(count_sens/len(preds)) > int(threshold/100):\n",
        "    print(\"=== MPC ===\")\n",
        "    mpc.run(mainmpc(file_path))\n",
        "  else:\n",
        "    print(\"=== REG ===\")\n",
        "    mainreg(file_path)\n",
        "\n",
        "main(\"./data/newtest.csv\", 50, 10) # Should use MPC\n",
        "main(\"./data/newtest2.csv\", 50, 10) # Should use Regular"
      ],
      "metadata": {
        "id": "AhJ6D6lhieS9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}